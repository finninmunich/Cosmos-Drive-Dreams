# Cosmos-Drive-Dreams
### [Paper]() | [Paper Website](https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams)
### [Models](#cosmos-drive-open-source-summary) | [Dataset](#cosmos-drive-dreams-dataset) | [Toolkits](#cosmos-drive-dreams-toolkits) | [SDG Pipeline](#cosmos-drive-dreams-sdg-pipeline)

This is the official code repository of Cosmos-Drive-Dreams - a Synthetic Data Generation (**SDG**) pipeline built on [Cosmos World Foundation Models](https://www.nvidia.com/en-us/ai/cosmos/) for generating diverse and challenging scenarios for Autonomous Vehicle use-cases. 

We open-source our **model weights**, **pipeline toolkits**, and a **dataset** (including cosmos-generated videos, paired HDMap and LiDAR), which consists of **81,802** clips.

<p align="center">
    <img src="assets/teaser.png" alt="Cosmos-Drive-Dream Teaser">
</p>

## News

- 2025-06-10: Model, Toolkits, and Dataset (including cosmos-generated video, HDMap, and LiDAR) are released! Stay tuned for the paired GT RGB videos. 


## Cosmos-Drive Open-source Summary
| **Name**                         | **Type**   | **Link** |
|----------------------------------|------------|----------|
| **Cosmos-7B-AV-Sample** (Paper Sec. [2.1])             | model     | [`base_model.pt`](https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV) |
| **Cosmos-7B-Multiview-AV-Sample** (Paper Sec. [2.1])             | model     | [Huggingface Link](https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World-Sample-AV-Multiview) |
| **Cosmos-Transfer1-7B-Sample-AV** (Paper Sec. [2.2]) | model     | [Huggingface Link](https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV) |
| **Cosmos-7B-Single2Multiview-Sample-AV** (Paper Sec. [2.3])     | model     | [Huggingface Link](https://huggingface.co/nvidia/Cosmos-Transfer1-7B-SingleToMultiView-Sample-AV) |
| **Cosmos-7B-Annotate-Sample-AV** (Paper Sec. [2.4])        | model     | Under review |
| **Cosmos-7B-LiDAR-GEN-Sample-AV** (Paper Sec. [3])         | model     | Under review |
| **Cosmos Toolkit**                | SDG tool   | [Cosmos Drive Dreams Toolkits](https://github.com/nv-tlabs/Cosmos-Drive-Dreams/tree/main/cosmos-drive-dreams-toolkits) |
| **RDS-HQ Dataset Subset**        | dataset    | [Huggingface Dataset](https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams) |
| **Cosmos-Drive-Dreams Dataset**     | dataset    | [Huggingface Dataset](https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams) |


## Cosmos-Drive-Dreams Dataset
Cosmos-Drive-Dreams Dataset contains labels (HDMap, BBox, and LiDAR) for 5,843 10-second clips collected by NVIDIA, along with 81,802 synthetic video samples generated by Cosmos-Drive-Dreams from these labels. The synthetically generated video is 121 frames long, capturing a wide variety of challenging scenarios, such as rainy, snowy, foggy, etc, that might not be as easily available in real-world driving datasets. This dataset is ready for commercial/non-commercial use.

Detailed information can be found on the [Huggingface page](https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams).

### Download
```bash
Coming soon
```

### Tutorial

- Visualizing the structured labels [here](https://github.com/nv-tlabs/Cosmos-Drive-Dreams/tree/main/cosmos-drive-dreams-toolkits#visualize-dataset)

## Cosmos-Drive-Dreams Toolkits

- Visualizing the structured labels [here](https://github.com/nv-tlabs/Cosmos-Drive-Dreams/tree/main/cosmos-drive-dreams-toolkits#visualize-dataset)

- Editing structured labels interactively to produce novel scenarios [here](https://github.com/nv-tlabs/Cosmos-Drive-Dreams/tree/main/cosmos-drive-dreams-toolkits#generate-novel-ego-trajectory)

- Converting Waymo Open Dataset to our format [here](https://github.com/nv-tlabs/Cosmos-Drive-Dreams/tree/main/cosmos-drive-dreams-toolkits#convert-public-datasets)

## Cosmos-Drive-Dreams SDG Pipeline
We provide a simple walkthrough including all stages of our SDG pipeline through example data available in the assets folder; no additional data download is necessary. 
For large-scale sampling, please download the above Cosmos-Drive-Dreams Dataset. 

### 0. Installation and Model Downloading
We recommend using conda for managing your environment. Detailed instructions for setting up Cosmos-Drive-Dreams can be found in [INSTALL.md](INSTALL.md).

### 1. Preprocessing Condition Videos
`cosmos-drive-dreams-toolkits/render_from_rds_hq.py` is used to render the HD map + bounding box / LiDAR condition videos from RDS-HQ dataset. 
In this example, we will only be rendering the HD map + bounding box condition videos.
Note that GPU is required for rendering LiDAR. 
```bash
cd cosmos-drive-dreams-toolkits

# generate multi-view condition videos.
# If you just want to generate front-view videos, replace `-d rds_hq_mv` with `-d rds_hq`
python render_from_rds_hq.py -i ../assets/example -o ../outputs -d rds_hq_mv --skip lidar
cd ..
```
This will automatically launch multiple jobs based on [Ray](https://docs.ray.io/en/releases-2.4.0/index.html) for data parallelization, but since we are only processing 1 clip here, it will only use 1 worker. The script should return in under a minute and produce a new directory at `outputs/hdmap`:
```bash
outputs/
└── hdmap/
    ├── ftheta_camera_cross_left_120fov
    │   └── 2d23a1f4-c269-46aa-8e7d-1bb595d1e421_2445376400000_2445396400000_0.mp4
    ├── ftheta_camera_cross_right_120fov
    │   └── 2d23a1f4-c269-46aa-8e7d-1bb595d1e421_2445376400000_2445396400000_0.mp4
    ├── ftheta_camera_front_wide_120fov
    │   └── 2d23a1f4-c269-46aa-8e7d-1bb595d1e421_2445376400000_2445396400000_0.mp4
    ├── ftheta_camera_rear_left_120fov
        └── 2d23a1f4-c269-46aa-8e7d-1bb595d1e421_2445376400000_2445396400000_0.mp4
    ...
```
The suffix `_0` means it is the first chunk of the video, which will be 121 frames long.

### 2. Prompt Rewriting
A prompt describing a possible manifestation for the example can be found in `assets/example/captions/2d23*.txt`. We can use a VLM ([Qwen3](https://github.com/QwenLM/Qwen3) to be exact) to augment this single prompt into many variations as follows: 
```bash
python scripts/rewrite_caption.py -i assets/example/captions -o outputs/captions
```
The output will be saved at `outputs/captions/2d23*json`.

### 3. Front-view Video Generation
Next, we use [Cosmos-Transfer1-7b-Sample-AV](https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV) to generate a 121-frame RGB video from the HD Map condition video and text prompt. 
```bash
PYTHONPATH="cosmos-transfer1" python scripts/generate_video_single_view.py --caption_path outputs/captions --input_path outputs --video_save_folder outputs/single_view --checkpoint_dir checkpoints/ --is_av_sample --controlnet_specs cosmos-transfer1/assets/sample_av_hdmap_spec.json
```
For detailed description on how to run this model and how to adjust inference parameters, see [this readme](https://github.com/nvidia-cosmos/cosmos-transfer1/blob/main/examples/inference_cosmos_transfer1_7b_sample_av.md).

### 4. Multiview Video Generation
After single view videos have been generated, we use [Cosmos-Transfer1-7b-Sample-AV-Single2MultiView](https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV)
```bash
CUDA_HOME=$CONDA_PREFIX PYTHONPATH="cosmos-transfer1" python scripts/generate_video_multi_view.py --caption_path outputs/captions --input_path outputs --input_view_path outputs/single_view --video_save_folder outputs/multi_view --checkpoint_dir checkpoints --is_av_sample --controlnet_specs cosmos-transfer1/assets/sample_av_hdmap_multiview_spec.json
```
For detailed description on how to run this model and how to adjust inference parameters, see [this readme](https://github.com/nvidia-cosmos/cosmos-transfer1/blob/main/examples/inference_cosmos_transfer1_7b_sample_av_single2multiview.md).

### 5. Filtering via VLM
Coming soon

## Citation
```bibtex
@misc{nvidia2025cosmostransfer1,
  title     = {Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models},
  author = {Ren, Xuanchi and Lu, Yifan and Cao, Tianshi and Gao, Ruiyuan and
          Huang, Shengyu and Sabour, Amirmojtaba and Shen, Tianchang and
          Pfaff, Tobias and Wu, Jay Zhangjie and Chen, Runjian and
          Kim, Seung Wook and Gao, Jun and Leal-Taixe, Laura and
          Chen, Mike and Fidler, Sanja and Ling, Huan}
  year      = {2025},
  url       = {https://arxiv.org/abs/}
}
```

```bibtex
@misc{nvidia2025cosmostransfer1,
  title     = {Cosmos Transfer1: World Generation with Adaptive Multimodal Control},
  author    = {NVIDIA}, 
  year      = {2025},
  url       = {https://arxiv.org/abs/2503.14492}
}
```


